{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{all of the folowing are multivariable quantities}:\n",
    "\\\\\n",
    "\\theta: \\text{unobservable vector quantities}\n",
    "\\\\\n",
    "y:\\text{observable data}\n",
    "\\\\\n",
    "\\tilde{y}:\\text{unknow, potentially observable quantities}\n",
    "\\\\\n",
    "p(\\cdot): \\text{probability of}\\ \\cdot\n",
    "\\\\\n",
    "p(\\cdot \\mid \\cdot): \\text{probability of}\\ \\cdot \\ \\text{given}\\ \\cdot\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to start talking about Bayesian, there's these simple concepts to learn: **prior distribution** $p(\\theta)$ represents our belief about the parameter $\\theta$ before seeing any data.\n",
    "The **likelihood function** or **sampling distribution** $p(y \\mid \\theta)$ tells us the probability of observing the data  $y$  given the parameter  $\\theta$.\n",
    "This takes us to the **joint probability distribution** for $\\theta\\;\\text{and}\\; y$:\n",
    "$$p(\\theta, y)=p(\\theta)p(y\\mid\\theta)$$\n",
    "\n",
    "which tells us that the probability of $\\theta$ **and** $y$ is equal the **prior distribution** of $\\theta$ multiplied by the **likelihod** of y given $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so:\n",
    "- $p(\\theta)$ **prior distribution**\n",
    "- $p(y\\mid\\theta)$ **likelihood function or sampling distribution**\n",
    "- $p(\\theta, y)=p(\\theta)p(y\\mid\\theta)$ **joint probability distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bayesian inference techniques specify how one should update one’s beliefs upon observing data. this will help me in the future trying to make machines think*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bayesian inference is used to calculate the **posterior distribution** $p(\\theta\\mid y)$ which is the probability of the parameter  $\\theta$  given the observed data y . Bayes’ rule is the key formula used to derive this:\n",
    "$$\n",
    "p(\\theta \\mid y)=\\frac{p(\\theta)p(y\\mid\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$p(\\theta \\mid y)$ is the posterior: what we want to calculate, the probability of $\\theta$ given the data $y$.\n",
    "\n",
    "$p(\\theta)$ is the prior: our belief about $\\theta$ before seeing any data.\n",
    "\n",
    "$p(y \\mid \\theta)$ is the likelihood: the probability of observing the data $y$ given a particular value of $\\theta$.\n",
    "\n",
    "$p(y)$ is the marginal likelihood (or evidence): the total probability of observing the data $y$, no matter what $\\theta$ is.\n",
    "    \n",
    "$p(y)$ is equal to $\\sum_\\theta p(\\theta)p(y\\mid\\theta)$ or $\\int p(\\theta)p(y\\mid\\theta)d\\theta$ for continuous variables, because $p(y)$ is the marginal likelihood, which is the total probability of observing the data  y , summed or integrated over all possible values of  \\theta .\n",
    "\n",
    "- to better understand the statements above i watched this: https://www.youtube.com/watch?v=HZGCoVF3YvM\n",
    "- used this visualization: https://seeing-theory.brown.edu/bayesian-inference/index.html\n",
    "- and lastly used this repo as example source: https://statswithr.github.io/book/the-basics-of-bayesian-statistics.html\n",
    "\n",
    "we can also have the form where p(y) is ommited because we have a fixed y, resulting in the **unormalized posterior density**:\n",
    "$$\n",
    "p(\\theta \\mid y)\\propto p(\\theta)p(y\\mid\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if i understand correctly, before using $y$ we need the distribution of this unknown data that is observable, this is called the **marginal distribution**:\n",
    "$$p(y) = \\int p(y, \\theta)d\\theta = \\int p(y)p(y\\mid\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "or **prior predictive distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have observed the data $y$, we can make a prediction about an unknown observable $\\tilde{y}$, like this:\n",
    "$$\n",
    "p(\\tilde{y}\\mid y) = \\int p(\\tilde{y},\\theta \\mid y)d\\theta\n",
    "\\\\\n",
    "\\int p(\\tilde{y}\\mid\\theta,y)p(\\theta\\mid y)d\\theta\n",
    "\\\\\n",
    "\\int p(\\tilde{y}\\mid\\theta)p(\\theta\\mid y)d\\theta\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
