{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{all of the folowing are multivariable quantities}:\n",
    "\\\\\n",
    "\\theta: \\text{unobservable vector quantities}\n",
    "\\\\\n",
    "y:\\text{observable data}\n",
    "\\\\\n",
    "\\tilde{y}:\\text{unknow, potentially observable quantities}\n",
    "\\\\\n",
    "p(\\cdot): \\text{probability of}\\ \\cdot\n",
    "\\\\\n",
    "p(\\cdot \\mid \\cdot): \\text{probability of}\\ \\cdot \\ \\text{given}\\ \\cdot\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to start talking about Bayesian, there's these simple concepts to learn: **prior distribution** $p(\\theta)$ represents our belief about the parameter $\\theta$ before seeing any data.\n",
    "The **likelihood function** or **sampling distribution** $p(y \\mid \\theta)$ tells us the probability of observing the data  $y$  given the parameter  $\\theta$.\n",
    "This takes us to the **joint probability distribution** for $\\theta\\;\\text{and}\\; y$:\n",
    "$$p(\\theta, y)=p(\\theta)p(y\\mid\\theta)$$\n",
    "\n",
    "which tells us that the probability of $\\theta$ **and** $y$ is equal the **prior distribution** of $\\theta$ multiplied by the **likelihod** of y given $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so:\n",
    "- $p(\\theta)$ **prior distribution**\n",
    "- $p(y\\mid\\theta)$ **likelihood function or sampling distribution**\n",
    "- $p(\\theta, y)=p(\\theta)p(y\\mid\\theta)$ **joint probability distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bayesian inference techniques specify how one should update one’s beliefs upon observing data. this will help me in the future trying to make machines think*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bayesian inference is used to calculate the **posterior distribution** $p(\\theta\\mid y)$ which is the probability of the parameter  $\\theta$  given the observed data y . Bayes’ rule is the key formula used to derive this:\n",
    "$$\n",
    "p(\\theta \\mid y)=\\frac{p(\\theta)p(y\\mid\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$p(\\theta \\mid y)$ is the posterior: what we want to calculate, the probability of $\\theta$ given the data $y$.\n",
    "\n",
    "$p(\\theta)$ is the prior: our belief about $\\theta$ before seeing any data.\n",
    "\n",
    "$p(y \\mid \\theta)$ is the likelihood: the probability of observing the data $y$ given a particular value of $\\theta$.\n",
    "\n",
    "$p(y)$ is the marginal likelihood (or evidence): the total probability of observing the data $y$, no matter what $\\theta$ is.\n",
    "    \n",
    "$p(y)$ is equal to $\\sum_\\theta p(\\theta)p(y\\mid\\theta)$ or $\\int p(\\theta)p(y\\mid\\theta)d\\theta$ for continuous variables, because $p(y)$ is the marginal likelihood, which is the total probability of observing the data  y , summed or integrated over all possible values of  $\\theta$ .\n",
    "\n",
    "- to better understand the statements above i watched this: https://www.youtube.com/watch?v=HZGCoVF3YvM\n",
    "\n",
    "- used this visualization: https://seeing-theory.brown.edu/bayesian-inference/index.html\n",
    "\n",
    "- and lastly used this repo as example source: https://statswithr.github.io/book/the-basics-of-bayesian-statistics.html\n",
    "\n",
    "we can also have the form where p(y) is ommited because we have a fixed y, resulting in the **unormalized posterior density**:\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid y)\\propto p(\\theta)p(y\\mid\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (the most used one)\n",
    "\n",
    "You have a medical test that can detect a disease, but it’s not perfect. Let’s say the disease is rare in the population, and the test is only somewhat reliable.\n",
    "- Prevalence of the disease: 1% of the population has the disease. This is the prior probability  **P(D)** .\n",
    "\n",
    "Test accuracy:\n",
    "- The test has a True Positive Rate (sensitivity) of **90%**. That means if you have the disease, there’s a 90% chance the test will come back positive.  **P(T|D) = 0.9**.\n",
    "- The test also has a False Positive Rate of **5%**. That means if you don’t have the disease, there’s a **5%** chance the test will still be positive.  **P(T|¬D) = 0.05**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability of you having a disease given a positive test is: 15.384615384615385%\n"
     ]
    }
   ],
   "source": [
    "# Define the probabilities\n",
    "P_D = 0.01  # Prior probability of having the disease (1%)\n",
    "P_T_given_D = 0.9  # True Positive Rate (sensitivity)\n",
    "P_T_given_not_D = 0.05  # False Positive Rate\n",
    "\n",
    "# Complementary probabilities\n",
    "P_not_D = 1 - P_D  # Probability of not having the disease\n",
    "\n",
    "# Calculate total probability of testing positive (P(T))\n",
    "P_T = (P_T_given_D * P_D) + (P_T_given_not_D * P_not_D)\n",
    "\n",
    "# Use Bayes' Theorem to calculate the posterior probability of having the disease given a positive test result (P(D|T))\n",
    "P_D_given_T = (P_T_given_D * P_D) / P_T\n",
    "\n",
    "print(f\"the probability of you having a disease given a positive test is: {P_D_given_T * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, as 3Blue1Brown explained in the video above, Bayesian inference involves selecting the area representing  P(D)  and refining it based on  P(T | D) . Then, all probabilities and their complements are summed to normalize the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if i understand correctly, before using $y$ we need the distribution of this unknown data that is observable, this is called the **marginal distribution**:\n",
    "$$p(y) = \\int p(y, \\theta)d\\theta = \\int p(y)p(y\\mid\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "or **prior predictive distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have observed the data $y$, we can make a prediction about an unknown observable $\\tilde{y}$, like this:\n",
    "$$\n",
    "p(\\tilde{y}\\mid y) = \\int p(\\tilde{y},\\theta \\mid y)d\\theta\n",
    "\\\\\n",
    "\\int p(\\tilde{y}\\mid\\theta,y)p(\\theta\\mid y)d\\theta\n",
    "\\\\\n",
    "\\int p(\\tilde{y}\\mid\\theta)p(\\theta\\mid y)d\\theta\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
